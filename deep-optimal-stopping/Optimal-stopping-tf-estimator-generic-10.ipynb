{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "74a2c9b6-abc9-48ad-832b-06ba7936fdd8"
    }
   },
   "source": [
    "### Deep Optimal Stopping - Implementation with tensorflow estimator\n",
    "\n",
    "#### General setting - demonstrated through the dice example\n",
    "The model implemented below is based on the paper \"Deep Optimal Stopping\" by Sebastian Becker, Patrick Cheridito & Arnulf Jentzen, see [link](https://arxiv.org/pdf/1804.05394.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "8e141512-a121-481e-a5b6-0fd4eb7eb5e6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import scipy\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a8537a15-5ae3-43a3-a38f-472795986af9"
    }
   },
   "source": [
    "#### Tensorflow estimators require input functions, one for training, and one for evaluation. We define these input functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "2b2cec99-a8a7-4042-aab5-b2c777048d74"
    }
   },
   "outputs": [],
   "source": [
    "# define input function for training with estimator, and use the dice np dataset \n",
    "def numpy_train_input_fn(samples): \n",
    "    # get the number of time steps and number of samples\n",
    "    n_timeSteps = np.shape(samples)[-1]\n",
    "    n_samples = np.shape(samples)[0]\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x = dict(zip(np.arange(n_timeSteps), np.reshape(samples.T.astype(float), \n",
    "                                                                             [n_timeSteps, n_samples, 1]))),\n",
    "        \n",
    "        batch_size = 64, \n",
    "        num_epochs = 30, \n",
    "        shuffle = True, \n",
    "        queue_capacity = 1000\n",
    "    )\n",
    "\n",
    "# define input function for evaluation\n",
    "def numpy_eval_input_fn(samples):\n",
    "    # get the number of time steps and number of samples\n",
    "    n_timeSteps = np.shape(samples)[-1]\n",
    "    n_samples = np.shape(samples)[0]\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "       x = dict(zip(np.arange(n_timeSteps), np.reshape(samples.T.astype(float), \n",
    "                                                                             [n_timeSteps, n_samples, 1]))),\n",
    "        \n",
    "        num_epochs = 1, \n",
    "        shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A key feature of the model architecture is that a simple *neural network* sits at each time point. Hence, if there is $n$ time steps, there will be $n-1$ neural nets; $n-1$, because at the terminal time, no neural net is needed. These networks are relatively simple, with two layers, and all have the same architecture. We will call such a network *gridNet* and it is defined next.  \n",
    "\n",
    "#### The *gridNet* function has the following inputs, \n",
    "- time point \n",
    "- inputs (that the network consumes)\n",
    "- nextInputs (this is needed to define the cost function, and it comes from the output of previous networks)\n",
    "- name (this is used to define the variable scope, which becomes important at training level, as training of such gridNets should be done separately, and sequentially);\n",
    "\n",
    "#### and outputs: \n",
    "\n",
    "- F_theta (needed for the cost function and training);\n",
    "- f_theta (needed for the calculation of stopping time);\n",
    "- gridCost (this is the cost function that needs to be optimised at a given time point).\n",
    "\n",
    "#### Note that the optimisers of these nets will be defined separately at the model/graph building stage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the individual networks\n",
    "# need: - the standard deviation of the initializers \n",
    "#       - learning rate of the individual optimizers; the learning rate will be given at a later stage\n",
    "\n",
    "stddev = 0.0005\n",
    "#alpha = 0.0008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the payoff function $g(\\cdot)$ for the general case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the payOff function func_g as a tensorflow function \n",
    "# for the dice example this function is the identity \n",
    "testId = lambda x: x \n",
    "\n",
    "def func_g(tens):\n",
    "    return tf.map_fn(testId, tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Define gridNet that sits on the time points\n",
    "# -------------------------------------------\n",
    "\n",
    "def gridNet(time_point, inputs, nextInputs, name):\n",
    "    one = tf.constant(1, dtype=tf.float64)\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        # architecture \n",
    "        first_layer = tf.layers.dense(inputs, 51, activation=tf.nn.relu,\n",
    "                                            #kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                            #kernel_initializer=tf.keras.initializers.he_normal(), \n",
    "                                            kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=stddev),\n",
    "                                            name=\"first_layer\")\n",
    "        second_layer = tf.layers.dense(first_layer, 51, activation=tf.nn.relu, \n",
    "                                            #kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                            #kernel_initializer=tf.keras.initializers.he_normal(), \n",
    "                                            kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=stddev),\n",
    "                                            name=\"second_layer\")\n",
    "        logits = tf.layers.dense(second_layer, 1, activation=None, \n",
    "                                          #kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                          #kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                          kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=stddev),\n",
    "                                          name=\"logits\") \n",
    "        F_theta = tf.nn.sigmoid(logits, name=\"F_theta\")\n",
    "    \n",
    "        # cost\n",
    "        \n",
    "        # ----------------------------------------------------------\n",
    "        # uncomment the lines below for the case with generic payoff\n",
    "        # ----------------------------------------------------------\n",
    "        #gridReward = tf.add(tf.multiply(F_theta, func_g(inputs)), \n",
    "        #                tf.multiply((one-F_theta), func_g(nextInputs)), \n",
    "        #               name = \"reward_\"+str(time_point))\n",
    "        \n",
    "        gridReward = tf.add(tf.multiply(F_theta, inputs), \n",
    "                        tf.multiply((one-F_theta), nextInputs), \n",
    "                       name = \"reward_\"+str(time_point))\n",
    "     \n",
    "        \n",
    "        gridCost = tf.scalar_mul(-1,tf.reduce_mean(gridReward))\n",
    "        \n",
    "    f_theta = tf.cast(tf.clip_by_value(tf.sign(logits), 0, 2), dtype=tf.int32, name=\"f_theta\")\n",
    "          \n",
    "    return [F_theta, f_theta, gridCost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the graph, which will serve as a model input for tensorflow estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Build the model \n",
    "# ---------------\n",
    "\n",
    "def my_model_fn(features, mode, params):  \n",
    "    \n",
    "    \"\"\"Defining the custom architecture\"\"\"\n",
    "    \n",
    "    N = len(features) # length of the time grid\n",
    "    \n",
    "    # creating the input set from the dictionary for practical considerations later\n",
    "    input_set = tf.concat([features[0], features[1]], 1, name=\"input_set\")    \n",
    "    for i in range(N-2):\n",
    "        input_set = tf.concat([input_set, features[2+i]], 1)\n",
    "    \n",
    "    # create dictionaries that, for each time point, store \n",
    "    taus = {}  # stopping times\n",
    "    nextInputs = {} # that go into the gridNets\n",
    "    ops={}  # optimizers\n",
    "    train_ops = {}  # training\n",
    "    NN = {}  # gridNets \n",
    "    \n",
    "    one = tf.constant(1, dtype=tf.float64)\n",
    "    \n",
    "    # define the networks and flows recursively, starting from N-2 and going to 0 \n",
    "    for t in range(N-2, -1, -1):\n",
    "        \n",
    "        if t==N-2:\n",
    "            nextInputs[t] = features[t+1]\n",
    "            NN[t] = gridNet(t, features[t], nextInputs[t], 'grid_'+str(t))        \n",
    "            ops[t] = tf.train.AdamOptimizer(learning_rate=params['alpha'][t])         \n",
    "            train_ops[t] = ops[t].minimize(NN[t][2], \n",
    "                                var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='grid_'+str(t))) \n",
    "                                          #global_step=tf.train.get_global_step())\n",
    "    \n",
    "            taus[t] = (N-2)*NN[t][1] + (N-1)*(1-NN[t][1])\n",
    "            \n",
    "        else:\n",
    "            nextInputs[t] = tf.gather_nd(input_set, \n",
    "                       indices=tf.concat([tf.reshape(tf.range(tf.shape(taus[t+1])[0]), shape=tf.shape(taus[t+1])), taus[t+1]], 1),\n",
    "                       name=\"g\"+str(t))\n",
    "            NN[t] = gridNet(t, features[t], nextInputs[t], 'grid_'+str(t))\n",
    "            ops[t] = tf.train.AdamOptimizer(learning_rate=params['alpha'][t])         \n",
    "            train_ops[t] = ops[t].minimize(NN[t][2], \n",
    "                                    var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='grid_'+str(t))) \n",
    "                                          #global_step=tf.train.get_global_step())\n",
    "    \n",
    "            taus[t] = tf.math.reduce_sum([t*NN[t][1]]+\n",
    "                    [i*NN[i][1]*tf.math.reduce_prod([(1-NN[j][1]) for j in range(t, i)], axis=0) for i in range((t+1),(N-1))]+\n",
    "                     [(N-1)*tf.math.reduce_prod([(1-NN[k][1]) for k in range(t, (N-1))], axis=0)]           \n",
    "                                , axis=0) \n",
    "        \n",
    "    # as a final step, pick the right element from the input set according to the stopping time taus[0]\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # uncomment the lines below for the case with generic payoff\n",
    "    # ----------------------------------------------------------\n",
    "    #args_tau = tf.gather_nd(input_set, \n",
    "    #                   indices=tf.concat([tf.reshape(tf.range(tf.shape(taus[0])[0]), shape=tf.shape(taus[0])), \n",
    "    #                                      taus[0]], 1), \n",
    "    #                   name=\"args_tau\")\n",
    "    \n",
    "    #gg_0 = func_g(args_tau)\n",
    "    \n",
    "    gg_0 = tf.gather_nd(input_set, \n",
    "                       indices=tf.concat([tf.reshape(tf.range(tf.shape(taus[0])[0]), shape=tf.shape(taus[0])), \n",
    "                                          taus[0]], 1), \n",
    "                       name=\"gg_0\")\n",
    "    \n",
    "    \n",
    "    # this will give the price -- the quantity we are looking for \n",
    "    price = tf.reduce_mean(gg_0, name=\"price\")\n",
    "    \n",
    "    # training params\n",
    "    global_step = tf.train.get_global_step()\n",
    "    update_global_step = tf.assign(global_step, global_step + 1, name = 'update_global_step')\n",
    "    \n",
    "    train_op =tf.group([train_ops[_] for _ in train_ops.keys()])\n",
    "    cost = tf.math.reduce_sum([NN[kk][2] for kk in train_ops.keys()]) #cost_1st+cost_2nd \n",
    "    \n",
    "    # estimator specs for EVAL \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=price,\n",
    "            evaluation_hooks=None)\n",
    "       \n",
    "    # estimator specs for PREDICT\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\"price\": price})\n",
    "    \n",
    "    # estimator specs for TRAINING\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            loss= price,\n",
    "            train_op= tf.group(train_op, update_global_step),\n",
    "            training_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aa25c9f7-6a01-4743-a0ab-e107e44a93c5"
    }
   },
   "source": [
    "#### Calculate the analytical prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "05ccf630-f923-4397-a1ee-0900f48b0d0a"
    }
   },
   "outputs": [],
   "source": [
    "#MODEL_DIR = '/Users/Cellini/Desktop/Quant/DL Udacity/DLexs/Estimator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that computes the analytical solution for given n steps \n",
    "def dicePrice(n):\n",
    "    prices = {}\n",
    "    \n",
    "    prices[1] = 3.5\n",
    "    if n==1: \n",
    "        return prices[1]\n",
    "    else:\n",
    "        for k in range(2,n+1):\n",
    "            prices[k] = (1./6.)*np.sum([max(prices[k-1], x) for x in [1, 2, 3, 4, 5, 6]])\n",
    "        return prices[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4.666666666666666)\n",
      "(4, 4.944444444444444)\n",
      "(5, 5.129629629629629)\n",
      "(6, 5.274691358024691)\n",
      "(7, 5.395576131687243)\n",
      "(8, 5.496313443072702)\n",
      "(9, 5.580261202560585)\n",
      "(10, 5.6502176688004875)\n"
     ]
    }
   ],
   "source": [
    "# print the analytical solutions\n",
    "for _ in range(3, 11):\n",
    "    print((_, dicePrice(_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating samples in the format of an $M \\times n$ matrix, where \n",
    "* $n$, the length of the row, represents the number of outcomes (or tosses), or the *time steps* in a time series;\n",
    "* $M$ represents the sample size. \n",
    "\n",
    "#### In the example below, we demonstrate the algorithm for $n=3, \\dots, 10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the run config\n",
    "config = tf.estimator.RunConfig(log_step_count_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x641fcbf28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-5-4d2ca92e038b>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.140625, step = 1\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1 vs previous value: 1. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 4 vs previous value: 4. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 9 vs previous value: 9. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 12 vs previous value: 12. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 18 vs previous value: 18. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 198.535\n",
      "INFO:tensorflow:loss = 4.390625, step = 2001 (10.074 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.8125.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:16:52Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /Users/Cellini/anaconda2/envs/DLexs/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:16:55\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 4.66699\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp7j24oo4i/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x642c5b7f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.328125, step = 1\n",
      "INFO:tensorflow:global_step/sec: 150.313\n",
      "INFO:tensorflow:loss = 5.203125, step = 2001 (13.306 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.015625.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:17:17Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:17:21\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 4.946731\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpwnkz7307/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x642ac4940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.46875, step = 1\n",
      "INFO:tensorflow:global_step/sec: 105.219\n",
      "INFO:tensorflow:loss = 5.0625, step = 2001 (19.009 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.28125.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:17:50Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:17:55\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.1295457\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp1udz1esb/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x6450fd438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.359375, step = 1\n",
      "INFO:tensorflow:global_step/sec: 101.26\n",
      "INFO:tensorflow:loss = 5.3125, step = 2001 (19.751 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.421875.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:18:29Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:18:35\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.276205\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp_a_p0sys/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x645150a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.46875, step = 1\n",
      "INFO:tensorflow:global_step/sec: 84.4378\n",
      "INFO:tensorflow:loss = 5.390625, step = 2001 (23.686 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.359375.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:19:15Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:19:21\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.4022436\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp3przuj4m/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x643829438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.53125, step = 1\n",
      "INFO:tensorflow:global_step/sec: 66.4072\n",
      "INFO:tensorflow:loss = 5.59375, step = 2001 (30.118 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.40625.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:20:10Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:20:17\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.4975724\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmp4ov_mi2l/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x643509d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.71875, step = 1\n",
      "INFO:tensorflow:global_step/sec: 67.8079\n",
      "INFO:tensorflow:loss = 5.546875, step = 2001 (29.496 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.375.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:21:07Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:21:15\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.58343\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpt4s3bctv/model.ckpt-2500\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13c6f4908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.59375, step = 1\n",
      "INFO:tensorflow:global_step/sec: 64.7586\n",
      "INFO:tensorflow:loss = 5.59375, step = 2001 (30.885 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.625.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-21T00:22:07Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-21-00:22:14\n",
      "INFO:tensorflow:Saving dict for global step 2500: global_step = 2500, loss = 5.646869\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/16/6vfzqvh50sv0n670v2qmktsw0000gn/T/tmpgzq24isb/model.ckpt-2500\n"
     ]
    }
   ],
   "source": [
    "M = 100000 # number of sample paths\n",
    "numberOfSteps = 2500 # number of training steps\n",
    "params = {} # dictionary (of parameters) that holds the learning rates \n",
    "\n",
    "lower = 3\n",
    "upper = 11\n",
    "\n",
    "# results that includes the price calculated by the model and the training times will be stored in\n",
    "# a dictionary, where the key indicated the number of time steps\n",
    "DOS = {}\n",
    "\n",
    "for i in range(lower, upper):\n",
    "    if i == 3:\n",
    "        params['alpha'] = [0.0008, 0.0008]\n",
    "    else:\n",
    "        params['alpha'] = np.linspace(0.0012, 0.0002, num=(i-1))\n",
    "    \n",
    "    dice = np.random.randint(low=1, high=7, size=(M, i)) # create samples for training\n",
    "    dice_eval = np.random.randint(low=1, high=7, size=(M, i)) # a separate sample for evaluation \n",
    "        \n",
    "    nn = tf.estimator.Estimator(#model_dir=MODEL_DIR, \n",
    "        model_fn=my_model_fn, params=params, config=config)\n",
    "    start = time.time()\n",
    "    nn.train(input_fn=numpy_train_input_fn(dice), steps=numberOfSteps)\n",
    "    end = time.time()\n",
    "    ev = nn.evaluate(input_fn=numpy_eval_input_fn(dice_eval))\n",
    "    \n",
    "    DOS[i] = [ev[\"loss\"], end-start]\n",
    "    \n",
    "    del dice, dice_eval, nn, ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "ea137665-bf31-431f-87fb-8bb13c3a0e14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3 -- Analytical Solution: 4.66667 -- ML Solution: 4.66699 -- Difference: 0.00032 -- Training time: 16.2470\n",
      "Steps: 4 -- Analytical Solution: 4.94444 -- ML Solution: 4.94673 -- Difference: 0.00229 -- Training time: 20.6288\n",
      "Steps: 5 -- Analytical Solution: 5.12963 -- ML Solution: 5.12955 -- Difference: 0.00008 -- Training time: 27.9400\n",
      "Steps: 6 -- Analytical Solution: 5.27469 -- ML Solution: 5.27621 -- Difference: 0.00151 -- Training time: 32.0522\n",
      "Steps: 7 -- Analytical Solution: 5.39558 -- ML Solution: 5.40224 -- Difference: 0.00667 -- Training time: 36.8816\n",
      "Steps: 8 -- Analytical Solution: 5.49631 -- ML Solution: 5.49757 -- Difference: 0.00126 -- Training time: 45.9945\n",
      "Steps: 9 -- Analytical Solution: 5.58026 -- ML Solution: 5.58343 -- Difference: 0.00317 -- Training time: 46.8430\n",
      "Steps: 10 -- Analytical Solution: 5.65022 -- ML Solution: 5.64687 -- Difference: 0.00335 -- Training time: 48.3600\n"
     ]
    }
   ],
   "source": [
    "# compare the results to the analytic solutions\n",
    "for i in range(lower, upper):\n",
    "    print(\"Steps: %d --\" %i, \"Analytical Solution: %.5f --\" %dicePrice(i), \"ML Solution: %.5f --\" %DOS[i][0], \"Difference: %.5f --\" %abs(dicePrice(i) - DOS[i][0]), \"Training time: %.4f\" %DOS[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1ec80917-86cf-47e6-81e2-6d765e42d00d"
    }
   },
   "source": [
    "#### Concluding remarks\n",
    "- The solver should produce reasonably accurate results, an error around $10^{-3}$, depending on the number of time steps. In the above setting, the first two decimals are expected to be correct, occasional slip ups, that is, greater errors, might appear in cases towards higher number of time steps, e.g. 8 or above. However even that is less expected in the current setting.\n",
    "- The largest number of time steps that was tested was $10$;\n",
    "- The standard deviation of the weight initializers in each individual network was set to the relatively low value of $\\sigma = 0.0005$;\n",
    "- Somewhat surprisingly, we found that taking different learning rates for the networks at each time step allows for a more accurate and reliable training results, and less training steps are needed. In the above setting, we found that a linearly decreasing learning rates works relatively well. More precisely, the last network has the smallest ($0.0002$) and the first network has the largest ($0.0012$) learning rate. The increments of the learning rate are linear and depend on the number of time steps; e.g. for the $10$ time steps case, the learning rates are set $[0.0012, 0.001075,  0.00095, 0.000825, 0.0007, 0.000575, 0.00045, 0.000325, 0.0002]$. The case of $3$ times steps, there are only two networks, and both learning rates are set to the same value of $0.0008$. Setting the learning rate the same for all networks can work reasonably well up until 8 time steps. We found that such setting breaks down, in terms of accuracy, for 9 and above number of time steps, and finding a good balance between number of training steps and learning rate becomes harder;  \n",
    "- Implementing the payoff function $g(\\cdot)$ that allows more general cases, slowed down the training times considerably. This is, likely, due to the follwoing two factors: 1) repeated function calls; 2) the payoff function appears in the optimisation process too. Currently that general setting is commented out. \n",
    "\n",
    "#### Potential directions for future work \n",
    "- It would be interesting to compare the performance of the solver to a more classical one, e.g. Longstaff-Schwartz American option pricer that is also based on MC methods. One could compare, whether the same samples lead to similar results, with similar variances. Even though there are other factors come into play, e.g. basis functions choice for the regression in the Longstaff-Schwartz method;\n",
    "- Analysing the performance bottle neck of the above solver and find a more efficient implementation, and/or training setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:DLexs]",
   "language": "python",
   "name": "conda-env-DLexs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
